{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scénario d'Apprentissage Statistique](https://github.com/wikistat/Apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation de données d'enquête en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> \n",
    "# Prévision du seuil de revenu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "Analyse du revenu ou plutôt du dépassement d'un seuil de revenu sur des données issues d'un sondage aux USA. Modélisation et prévision du dépassement d'un seuil de revenu. Comparaison de la pertinence et de l'efficacité de différentes méthodes d emodélisation ou apprentissage.\n",
    "## Introduction\n",
    "Des données publiques disponibles sur le site [UCI repository](http://archive.ics.uci.edu/ml/) sont extraites de la base de données issue du recensement réalisé aux Etats Unis en 1994. Ces données son largement utilisées et font référence comme outil de *benchmark* pour comparer les performances de méthodes d’apprentissage ou modélisation statistique. L’objectif est  de prévoir la variable binaire « revenu annuel » (`income`) supérieur ou inférieur à 50k$. Il ne s’agit pas encore de données massives mais 32561 individus sont décrits par les 14 variables du tableau ci-dessous. La phase préliminaire de préparation et exploration des données est décrite dans un scénario de la [Saison 2: Exploration](https://github.com/wikistat/Exploration). \n",
    "\n",
    "Num| Libellé |\tEnsemble de valeurs\n",
    "-|--|--|--\n",
    "1|`Age`|\treal\n",
    "2|\t`workClass`|\tPrivate, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "3|\t`fnlwgt`|\treal\n",
    "4|\t`education`|\tBachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool\n",
    "5|\t`educNum`|\tinteger\n",
    "6|\t`mariStat`|\tMarried-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n",
    "7|\t`occup`|\tTech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n",
    "8|\t`relationship`|\tWife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "9|\t`origEthn`|\tWhite, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "10|\t`sex`|\tFemale, Male\n",
    "11|\t`capitalGain`|\treal  \n",
    "12|\t`capitalLoss`|\treal\n",
    "13|\t`hoursWeek`|\treal\n",
    "14|\t`nativCountry`|\tUnited-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\n",
    "15|\t`income`|\t\tincHigh (>50K), incLow (<=50K)\n",
    "\n",
    "Une première étape permettant de vérifier, sélectionner, recoder les données, a permis de construire un fichier de type `.csv` qui est utilisé dans ce calepin.\n",
    "\n",
    "\n",
    "**Répondre aux questions en s'aidant des résultats des exécutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Préparation des données\n",
    "### 1.1 Lecture  \n",
    "\n",
    "Charger les [données](https://www.math.univ-toulouse.fr/~besse/Wikistat/data/adultCensus.csv)  dans le répertoire courant (`path=\"\"`) ou exécuter directement la cellule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path=\"\"  # si les données sont déjà dans le répertoire courant\n",
    "#path=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/\"\n",
    "datBase=pd.read_csv(path+'adultCensus.csv')\n",
    "datBase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repérer les variables quantitatives, qualitatives. Certaines (`age`, `hoursWeek`) sont présentes sous les deux types. Beaucoup de modalités on déjà été regroupées (voir le programme) certaines variables sont rendues qualitatives (`capitalLoss` ou `Gain`) et transformées par une fonction *log*. La variable à modéliser est `income`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 Exploration élémentaire et vérifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beaucoup de variables qualitatives\n",
    "datBase.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pas de données manquantes\n",
    "datBase.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la distribution de la variable `age`, de celle `income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase[\"age\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase[\"fnlwgt\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase[\"relationship\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase.plot(kind=\"scatter\",x=\"age\",y=\"educNum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire des liaisons : `age x hoursWeek`, `age x income`, `sex x income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase.plot(kind=\"scatter\",x=\"hoursWeek\",y=\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase.boxplot(column=\"age\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** La variable `fnlwgt` (Final sampling weight:  Inverse of sampling fraction adjusted for non-response and over or under sampling of particular groups) est assez obscure.  Que dire de sa liaison avec la variable cible? Elle est supprimée par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBase.boxplot(column=\"fnlwgt\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mosaic plot\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "mosaic(datBase,[\"income\",\"sex\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Variables indicatrices\n",
    "**Q** Pourquoi l’introduction de dummy variables ? Pour quelles méthodes cela serait-il aussi nécessaire en R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datBaseDum=pd.get_dummies(datBase[[\"workClass\",\"education\",\"mariStat\",\n",
    "    \"occup\",\"relationship\",\"origEthn\",\"sex\",\"capitalGain\",\"capitalLoss\",\n",
    "    \"nativCountry\",\"ageQ\",\"hoursWeekQ\"]])\n",
    "datBaseDum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datBase[[\"age\",\"educNum\",\"hoursWeek\",\"LcapitalGain\",\n",
    "             \"LcapitalLoss\",\"income\"]].join(datBaseDum)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "Y=datBase[\"income\"]\n",
    "# Variables prédictives\n",
    "X=X.drop([\"income\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Extraction des échantillons\n",
    "**Q** Quel est l’objectif de cette cellule ? Justifier la nécessité de ce traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=3000,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Méthodes de modélisation \n",
    "L'algorithme des *k* plus proches voisins fourni des réultats assez catastrophiques sur ces données, il n'est pas repris. Est-ce dû au nombre important d'indicatrices dans les variables explicatives? A confirmer sur d'autres exemples.\n",
    "### 2.1 [Régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "**Q** Commenter  les options de la commande  `GridSearchCV`. A quoi sert `param` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "tps0=time.clock()\n",
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "param=[{\"C\":[0.1, 0.15,0.2,0.25,0.3]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\"), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train, Y_train)  # GridSearchCV est lui même un estimateur\n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps logit = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                              1.-logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur sur l'échantillon test\n",
    "1-logitOpt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prévision\n",
    "y_chap = logitOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** La matrice de confusion n’est pas symétrique. Quelle pourrait en être la raison ?\n",
    "\n",
    "**Q** Quels algorithmes pourraient être exécutés en R pour la régression logistique? Quelles informations complémentaires en tirer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "LogisticRegression(penalty=\"l1\",C=logitOpt.best_params_['C']).fit(X_train, Y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "model = LogisticRegressionCV(Cs=10,cv=5, penalty=\"l1\",\n",
    "        n_jobs=-1,random_state=13,solver=\"liblinear\").fit(X_train,Y_train)\n",
    "m_log_alphas = -np.log10(model.Cs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(m_log_alphas, model.scores_['incLow'].T, ':')\n",
    "plt.plot(m_log_alphas, model.scores_['incLow'].T.mean(axis=-1), 'k',\n",
    "         label='precision moyenne', linewidth=2)\n",
    "plt.axvline(-np.log10(model.C_), linestyle='--', color='k',\n",
    "            label='Cs: optimal par VC')\n",
    "plt.legend()\n",
    "plt.xlabel('-log(CS)')\n",
    "plt.ylabel(u'Précision')\n",
    "plt.title(u'Précision de chaque validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que représente le graphique ? Comment l’interpréter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 [Arbre binaire de discrimination](http://wikistat.fr/pdf/st-m-app-cart.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tps0=time.clock()\n",
    "# Optimisation de la profondeur de l'arbre\n",
    "param=[{\"max_depth\":list(range(2,10))}]\n",
    "tree= GridSearchCV(DecisionTreeClassifier(),param,cv=10,n_jobs=-1)\n",
    "treeOpt=tree.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps arbre = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                             1. - treeOpt.best_score_,treeOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision\n",
    "1-treeOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = treeOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeOpt.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus\n",
    "treeG=DecisionTreeClassifier(max_depth=treeOpt.best_params_['max_depth'])\n",
    "treeG.fit(X_train,Y_train)\n",
    "dot_data = StringIO() \n",
    "export_graphviz(treeG, out_file=dot_data) \n",
    "graph=pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"treeOpt.png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='treeOpt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle est l’insuffisance de l’implémentation des arbres de décision dans Scikit-learn  par rapport à celle de rpart de R ? Que dire de l’arbre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 [Perceptron](http://wikistat.fr/pdf/st-m-app-rn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xnet_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xnet_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps0=time.clock()\n",
    "param_grid=[{\"hidden_layer_sizes\":list([(5,),(6,),(7,),(8,)])}]\n",
    "nnet= GridSearchCV(MLPClassifier(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(Xnet_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps perceptron = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle stratégie d’optimisation est adoptée ? Quelle autre pourrait l’être? Quel réseau pourrait également être pris en compte? Quelles sont les fonctions d’activation des neurones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision sur le test\n",
    "1-nnetOpt.score(Xnet_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = nnetOpt.predict(Xnet_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 [Forêts aléatoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "**Q** Commenter les choix de tous les paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "rfFit = forest.fit(X_train,Y_train)\n",
    "print(1-rfFit.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfFit.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation du paramètre\n",
    "tps0=time.clock()\n",
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=10,n_jobs=-1)\n",
    "rfOpt=rf.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps r forest = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision\n",
    "y_chap = rfFit.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(n_estimators=100,max_features=2)\n",
    "rfFit=rf.fit(X_train, Y_train)\n",
    "# Importance décroissante des variables\n",
    "importances = rfFit.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(20):\n",
    "    print(X_train.columns[indices[f]], importances[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphe des importances\n",
    "plt.figure()\n",
    "plt.title(\"Importances des variables\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment est obtenu le graphique ? Quelle importance ? Comment interpréter ces résultats ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 [Gradient boosting](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "**Q** Pourquoi pas de paramètre `njobs=-1`? \n",
    "**Q** En plus de celui `optimiser, quels sont les 2 principaux paramètres cet algorithme laissés par défaut ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "tps0=time.clock()\n",
    "param=[{\"n_estimators\":[100, 150, 200, 250]}]\n",
    "gbm= GridSearchCV(GradientBoostingClassifier(),param,cv=10)\n",
    "gbmOpt=gbm.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps boosting = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                        1. - gbmOpt.best_score_,gbmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-gbmOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = gbmOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Comparaison des méthodes\n",
    "\n",
    "### 3.1 [Courbes ROC](http://wikistat.fr/pdf/st-m-app-risque.pdf)\n",
    "**Q** En cohérence avec les résultats précédents, quelle est la courbe la plus au dessus des autres? Commenter ce graphique, que signifie AUC ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod=[[\"GBM\",gbmOpt],[\"RF\",rfOpt],[\"NN\",nnetOpt],[\"Tree\",treeOpt],[\"Logit\",logitOpt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles tous réajustés sur les mêmes variables centrées réduites\n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].fit(Xnet_train, Y_train).predict_proba(Xnet_test)\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,probas_[:,1], pos_label=\"incLow\")\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0]),\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [Validation croisée](http://wikistat.fr/pdf/st-m-app-risque.pdf) *Monte Carlo*\n",
    "**Q** Quelle différence entre la validation croisée Monte Carlo et la V-fold cross validation?\n",
    "\n",
    "**Q** Les variables sont « standardisées ». Pourquoi ? Est-ce important et pour quelles méthodes? Commenter les résultats. Quelle méthode choisir ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "import time\n",
    "tps0=time.clock()\n",
    "check_random_state(11)\n",
    "# définition des estimateurs\n",
    "logit= LogisticRegression(penalty=\"l1\")\n",
    "tree = DecisionTreeClassifier()\n",
    "nnet = MLPClassifier(max_iter=400)\n",
    "rf   = RandomForestClassifier(n_estimators=200)\n",
    "gbm  = GradientBoostingClassifier()\n",
    "# Nombre d'itérations\n",
    "B=3 # pour utiliser le programme, mettre plutôt B=10\n",
    "# définition des grilles de paramètres\n",
    "listMethGrid=[[gbm,{\"n_estimators\":[100, 150, 200, 250]}],\n",
    "    [rf,{\"max_features\":list(range(6,10))}],\n",
    "    [nnet,{\"hidden_layer_sizes\":list([(5,),(6,),(7,),(8,)])}],\n",
    "    [tree,{\"max_depth\":list(range(2,10))}],\n",
    "    [logit,{\"C\":[0.1, 0.15,0.2,0.25,0.3]}]]\n",
    "# Initialisation à 0 \n",
    "arrayErreur=np.empty((B,5))\n",
    "for i in range(B):   # itérations sur B échantillons test\n",
    "    # extraction apprentissage et test\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=1000)\n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X_train)  \n",
    "    Xnet_train = scaler.transform(X_train)  \n",
    "    # Meme transformation sur le test\n",
    "    Xnet_test = scaler.transform(X_test)\n",
    "    # optimisation de chaque méthode et calcul de l'erreur sur le test\n",
    "    for j,(method, grid_list) in enumerate(listMethGrid):\n",
    "        methodGrid=GridSearchCV(method,grid_list,cv=10,n_jobs=-1).fit(X_train, Y_train)\n",
    "        methodOpt = methodGrid.best_estimator_\n",
    "        methFit=methodOpt.fit(Xnet_train, Y_train)\n",
    "        arrayErreur[i,j]=1-methFit.score(Xnet_test,Y_test)\n",
    "tps1=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeErreur=pd.DataFrame(arrayErreur,columns=[\"GBM\",\"RF\",\"NN\",\"Tree\",\"Logit\"])\n",
    "print(\"Temps execution en mn :\",(tps1 - tps0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeErreur[[\"GBM\",\"RF\",\"NN\",\"Tree\",\"Logit\"]].boxplot(return_type='dict')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes\n",
    "dataframeErreur.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Q** Les SVM ne font pas partie de la comparaison à cause de temps rédhibitoires d’exécution. A quoi est-ce dû ? Commenter les temps d’exécution des différentes étapes. Quelle autre package pourrait être utilisé pour la section 2.5 ? Pourquoi ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
